{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMCLcavQNONNfIT/4fKlMRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/sentiment_analysis/blob/main/sentiment_analysis_bert_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsptrsxA2r6g"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "dT1HfCDFAGm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Normalize unicode characters\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove special characters (optional)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "def load_and_process(file, training):\n",
        "  df = pd.read_csv(file, names=['id', 'entity', 'label', 'text']).set_index('id')\n",
        "  df = df.drop_duplicates().dropna()\n",
        "  df['text_processed'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "  if training:\n",
        "      df['label_num'] = label_encoder.fit_transform(df['label'])\n",
        "  else:\n",
        "      df['label_num'] = label_encoder.transform(df['label'])\n",
        "\n",
        "  return df\n",
        "\n",
        "train_df = load_and_process(\"/content/twitter_training.csv\", True)\n",
        "test_df = load_and_process(\"/content/twitter_validation.csv\", False)"
      ],
      "metadata": {
        "id": "Ueut0n8L34rA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "gzJPNdSfAJxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertModel.from_pretrained(model_name)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert_model.to(device)\n",
        "\n",
        "def get_bert_embedding(sentence):\n",
        "    tokens = tokenizer(sentence, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    for k,v in tokens.items():\n",
        "        tokens[k] = v.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**tokens)\n",
        "\n",
        "    embedding = outputs.pooler_output.squeeze().cpu().numpy()\n",
        "    return embedding\n",
        "X_test = np.array([get_bert_embedding(sentence) for sentence in test_df[\"text_processed\"]])\n",
        "X_train = np.array([get_bert_embedding(sentence) for sentence in train_df[\"text_processed\"]])"
      ],
      "metadata": {
        "id": "5X42hmY44Lox"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = train_df['label_num'].to_numpy()\n",
        "Y_test = test_df['label_num'].to_numpy()"
      ],
      "metadata": {
        "id": "HhV03ve3556s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mode Training & Evaluation"
      ],
      "metadata": {
        "id": "aNCe3ZXCANHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
        "    'SVM': LinearSVC(),\n",
        "}\n",
        "for model in models.values():\n",
        "    model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "03J67EqZ8wmz",
        "outputId": "2c14a053-e08c-43c2-c437-543c0b253109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in models.items():\n",
        "    Y_predicts = model.predict(X_test)\n",
        "    report = classification_report(Y_test, Y_predicts, target_names=label_encoder.classes_)\n",
        "    print(name)\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "fidmtiS97Bmz",
        "outputId": "38ce799c-602c-4a3c-9d44-549748bdc5a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.61      0.30      0.40       172\n",
            "    Negative       0.60      0.74      0.66       266\n",
            "     Neutral       0.57      0.58      0.58       285\n",
            "    Positive       0.63      0.67      0.65       277\n",
            "\n",
            "    accuracy                           0.60      1000\n",
            "   macro avg       0.60      0.57      0.57      1000\n",
            "weighted avg       0.60      0.60      0.59      1000\n",
            "\n",
            "SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.63      0.26      0.36       172\n",
            "    Negative       0.60      0.77      0.68       266\n",
            "     Neutral       0.56      0.59      0.57       285\n",
            "    Positive       0.64      0.66      0.65       277\n",
            "\n",
            "    accuracy                           0.60      1000\n",
            "   macro avg       0.61      0.57      0.57      1000\n",
            "weighted avg       0.60      0.60      0.59      1000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}