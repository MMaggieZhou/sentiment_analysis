{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPQ696Leiscig4piHvpJMJM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/sentiment_analysis/blob/main/sentiment_analysis_bert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Leveraging Pertrained Bert via Transformers Library from Hugging Face"
      ],
      "metadata": {
        "id": "8ph8sLf1oTaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import torch\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "rNqmCIdSmzXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Tokenizer provided by transformers utilises sub-word tonization, so very little data cleaning is needed, aka convert to lower case, normalize unicode characters, as well as remove special characters."
      ],
      "metadata": {
        "id": "u-2nN_LnngN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Normalize unicode characters\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove special characters (optional)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "def load_and_process(file, training):\n",
        "  df = pd.read_csv(file, names=['id', 'entity', 'label', 'text']).set_index('id')\n",
        "  df = df.drop_duplicates().dropna()\n",
        "  df['text_processed'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "  if training:\n",
        "      df['label_num'] = label_encoder.fit_transform(df['label'])\n",
        "  else:\n",
        "      df['label_num'] = label_encoder.transform(df['label'])\n",
        "\n",
        "  return df\n",
        "\n",
        "train_df = load_and_process(\"/content/twitter_training.csv\", True)\n",
        "test_df = load_and_process(\"/content/twitter_validation.csv\", False)"
      ],
      "metadata": {
        "id": "OwUHRN4H-dky"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Use Tokenizer provided by transformers, to convert text into pytorch tensors, and further into Dataset as the desired format for training."
      ],
      "metadata": {
        "id": "B0KBUt3qnvEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_texts(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\" # Return PyTorch torch.Tensor objects.\n",
        "    )\n",
        "train_encodings = tokenize_texts(train_df['text_processed'], tokenizer)\n",
        "val_encodings = tokenize_texts(test_df['text_processed'], tokenizer)\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        # encodings : output from BertTokenizer\n",
        "        self.encodings = encodings\n",
        "        # labes: df\n",
        "        self.labels = torch.tensor(labels.values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # encoding shoud have keys{'input_ids', 'attention_mask'}\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "train_dataset = TweetDataset(train_encodings, train_df['label_num'])\n",
        "val_dataset = TweetDataset(val_encodings, test_df['label_num'])"
      ],
      "metadata": {
        "id": "5_2AhPavCwZz"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Pretrained Model Loading & Fine-tune\n",
        "\n",
        "Load pre-trained bert model into [BertForSequenceClassification](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L1631), which basically add a classification head to bert model.\n",
        "\n",
        "Use the provided Trainer class for training and evaluation, which uses AdamW as optimizer by default, and utilizers GPU/TPU when available."
      ],
      "metadata": {
        "id": "bn9qDvHcnzrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
        "\n",
        "# Evaluation\n",
        "def compute_metrics(pred):\n",
        "    logits = pred.predictions\n",
        "    targets = pred.label_ids\n",
        "    preds = torch.argmax(torch.tensor(logits), axis=-1)\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Log locally, turns out this makes training faster\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# The default optimizer is AdamW\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(next(trainer.model.parameters()).device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih2S2URQFVfk",
        "outputId": "adb9d897-cdd6-4a13-ba45-ac9374cd8761"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "oEvb0YoPN2aJ",
        "outputId": "b9e0123e-e202-4bae-878e-c777d2639e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13305' max='13305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13305/13305 19:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.523800</td>\n",
              "      <td>0.262728</td>\n",
              "      <td>0.912000</td>\n",
              "      <td>0.913682</td>\n",
              "      <td>0.912000</td>\n",
              "      <td>0.911685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.226000</td>\n",
              "      <td>0.105904</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.970104</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.970021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.112000</td>\n",
              "      <td>0.135919</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.970120</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>0.970013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=13305, training_loss=0.4060885070544891, metrics={'train_runtime': 1174.9732, 'train_samples_per_second': 181.173, 'train_steps_per_second': 11.324, 'total_flos': 1.4002627143038976e+16, 'train_loss': 0.4060885070544891, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ]
}